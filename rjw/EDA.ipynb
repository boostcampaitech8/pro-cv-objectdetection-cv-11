{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "# faster rcnn model이 포함된 library\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    coco: COCO(annotation)으로 미리 생성한 객체\n",
    "    img_ids: 학습에 사용할 이미지 id 리스트\n",
    "    data_dir: 이미지가 존재하는 폴더 경로\n",
    "    transforms: Albumentations Compose\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, coco, img_ids, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.coco = coco\n",
    "        self.img_ids = img_ids\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # 예측 결과 저장용(기존 코드 유지)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.img_ids[index]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        H, W = image.shape[:2]\n",
    "        min_area = 16.0   # 필요하면 32, 64 등으로 조정\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        is_crowds = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "\n",
    "            # 1) width/height가 0 이하이면 제거\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "\n",
    "            # COCO → Pascal VOC\n",
    "            x_min = x\n",
    "            y_min = y\n",
    "            x_max = x + w\n",
    "            y_max = y + h\n",
    "\n",
    "            # 2) 이미지 경계로 클리핑\n",
    "            x_min = max(0, min(x_min, W - 1))\n",
    "            y_min = max(0, min(y_min, H - 1))\n",
    "            x_max = max(0, min(x_max, W - 1))\n",
    "            y_max = max(0, min(y_max, H - 1))\n",
    "\n",
    "            # 3) 클리핑 후 다시 유효성 검사\n",
    "            if x_max <= x_min or y_max <= y_min:\n",
    "                continue\n",
    "\n",
    "            # 4) 너무 작은 박스 제거 (area 기준)\n",
    "            box_area = (x_max - x_min) * (y_max - y_min)\n",
    "            if box_area < min_area:\n",
    "                continue\n",
    "\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'] + 1)          # 1~10\n",
    "            areas.append(box_area)                         # 클리핑 후 면적\n",
    "            is_crowds.append(int(ann.get('iscrowd', 0)))\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "            labels = np.zeros((0,), dtype=np.int64)\n",
    "            areas = np.zeros((0,), dtype=np.float32)\n",
    "            is_crowds = np.zeros((0,), dtype=np.int64)\n",
    "        else:\n",
    "            boxes = np.array(boxes, dtype=np.float32)\n",
    "            labels = np.array(labels, dtype=np.int64)\n",
    "            areas = np.array(areas, dtype=np.float32)\n",
    "            is_crowds = np.array(is_crowds, dtype=np.int64)\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([image_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': is_crowds\n",
    "        }\n",
    "\n",
    "        if self.transforms and len(boxes) > 0:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.tensor(sample['bboxes'],\n",
    "                                        dtype=torch.float32)\n",
    "            target['labels'] = torch.as_tensor(sample['labels'],\n",
    "                                            dtype=torch.int64)\n",
    "        else:\n",
    "            image = torch.tensor(image).permute(2, 0, 1).float()\n",
    "            target['boxes'] = torch.tensor(target['boxes'],\n",
    "                                        dtype=torch.float32)\n",
    "\n",
    "        return image, target, image_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(1024, 1024),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for out, img_id in zip(outputs, image_ids):\n",
    "                boxes = out['boxes'].cpu().numpy()\n",
    "                scores = out['scores'].cpu().numpy()\n",
    "                labels = out['labels'].cpu().numpy()\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    w = x_max - x_min\n",
    "                    h = y_max - y_min\n",
    "                    results.append({\n",
    "                        \"image_id\": int(img_id),\n",
    "                        \"category_id\": int(label - 1),     # 1~10 → 0~9\n",
    "                        \"bbox\": [float(x_min), float(y_min), float(w), float(h)],\n",
    "                        \"score\": float(score),\n",
    "                    })\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()       # mAP, mAP50 등 출력\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(num_epochs, train_loader, val_loader, coco_val,\n",
    "             optimizer, model, device):\n",
    "    best_loss = float(\"inf\")\n",
    "    loss_hist = Averager()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_hist.reset()\n",
    "        model.train()\n",
    "\n",
    "        for images, targets, image_ids in tqdm(train_loader):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            loss_hist.send(loss_value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch #{epoch+1} train loss: {loss_hist.value:.4f}\")\n",
    "\n",
    "        # validation mAP\n",
    "        evaluate(model, val_loader, coco_val, device)\n",
    "\n",
    "        # best 모델 저장 (loss 기준 or 나중에 mAP 기준으로 변경 가능)\n",
    "        if loss_hist.value < best_loss:\n",
    "            os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "            torch.save(model.state_dict(),\n",
    "                       \"./checkpoints/faster_rcnn_best.pth\")\n",
    "            best_loss = loss_hist.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image weighting for rare class sampling\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def make_image_weights(annotation_path, rare_ratio=0.1,\n",
    "                       rare_weight=3.0, normal_weight=1.0):\n",
    "    coco = COCO(annotation_path)\n",
    "    anns = coco.dataset[\"annotations\"]\n",
    "\n",
    "    # 클래스별 box 개수 집계\n",
    "    cls_count = defaultdict(int)\n",
    "    for ann in anns:\n",
    "        cid = ann[\"category_id\"]\n",
    "        cls_count[cid] += 1\n",
    "\n",
    "    max_cnt = max(cls_count.values())\n",
    "    # 상위 빈도의 rare_ratio 비율 미만이면 희귀 클래스로 정의\n",
    "    rare_classes = {c for c, n in cls_count.items()\n",
    "                    if n < rare_ratio * max_cnt}\n",
    "\n",
    "    # 이미지별 포함 클래스 집합\n",
    "    img_to_cats = defaultdict(set)\n",
    "    for ann in anns:\n",
    "        img_to_cats[ann[\"image_id\"]].add(ann[\"category_id\"])\n",
    "\n",
    "    img_ids = coco.getImgIds()\n",
    "    img_weights = []\n",
    "    for img_id in img_ids:\n",
    "        cats = img_to_cats.get(img_id, set())\n",
    "        # 희귀 클래스가 하나라도 있으면 더 큰 weight 부여\n",
    "        if len(cats & rare_classes) > 0:\n",
    "            img_weights.append(rare_weight)\n",
    "        else:\n",
    "            img_weights.append(normal_weight)\n",
    "\n",
    "    return coco, img_ids, torch.DoubleTensor(img_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_weights(annotation_path,\n",
    "                       rare_ratio=0.1,\n",
    "                       rare_weight=3.0,\n",
    "                       normal_weight=1.0):\n",
    "    \"\"\"\n",
    "    COCO annotation을 읽어서\n",
    "    - 클래스별 box 개수\n",
    "    - 희귀 클래스(rare_classes)\n",
    "    - 이미지별 weight (희귀 클래스 포함 시 rare_weight)\n",
    "    를 계산.\n",
    "    \"\"\"\n",
    "    coco = COCO(annotation_path)\n",
    "    anns = coco.dataset[\"annotations\"]\n",
    "\n",
    "    cls_count = defaultdict(int)\n",
    "    for ann in anns:\n",
    "        cid = ann[\"category_id\"]\n",
    "        cls_count[cid] += 1\n",
    "\n",
    "    max_cnt = max(cls_count.values())\n",
    "    rare_classes = {c for c, n in cls_count.items()\n",
    "                    if n < rare_ratio * max_cnt}\n",
    "\n",
    "    img_to_cats = defaultdict(set)\n",
    "    for ann in anns:\n",
    "        img_to_cats[ann[\"image_id\"]].add(ann[\"category_id\"])\n",
    "\n",
    "    img_ids = coco.getImgIds()\n",
    "\n",
    "    img_weights = []\n",
    "    for img_id in img_ids:\n",
    "        cats = img_to_cats.get(img_id, set())\n",
    "        if len(cats & rare_classes) > 0:\n",
    "            img_weights.append(rare_weight)\n",
    "        else:\n",
    "            img_weights.append(normal_weight)\n",
    "\n",
    "    img_weights = torch.DoubleTensor(img_weights)\n",
    "    return coco, img_ids, img_weights\n",
    "\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        weights=\"DEFAULT\"\n",
    "    )\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features,\n",
    "                                                      num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loaders():\n",
    "    # split해서 만든 json 사용\n",
    "    train_ann = '../../dataset/train_split.json'\n",
    "    val_ann   = '../../dataset/val_split.json'\n",
    "    data_dir  = '../../dataset'\n",
    "\n",
    "    # ---- train loader (샘플링 포함) ----\n",
    "    coco_train, train_ids, img_weights = make_image_weights(\n",
    "        annotation_path=train_ann,\n",
    "        rare_ratio=0.1,\n",
    "        rare_weight=3.0,\n",
    "        normal_weight=1.0\n",
    "    )\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        coco=coco_train,\n",
    "        img_ids=train_ids,\n",
    "        data_dir=data_dir,\n",
    "        transforms=get_train_transform()\n",
    "    )\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=img_weights,\n",
    "        num_samples=len(img_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # ---- val loader (샘플링 없음) ----\n",
    "    coco_val = COCO(val_ann)\n",
    "    val_ids = coco_val.getImgIds()\n",
    "\n",
    "    val_dataset = CustomDataset(\n",
    "        coco=coco_val,\n",
    "        img_ids=val_ids,\n",
    "        data_dir=data_dir,\n",
    "        transforms=get_valid_transform()\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, coco_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "def evaluate(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for out, img_id in zip(outputs, image_ids):\n",
    "                boxes = out['boxes'].cpu().numpy()\n",
    "                scores = out['scores'].cpu().numpy()\n",
    "                labels = out['labels'].cpu().numpy()\n",
    "\n",
    "                # label: 1~10 → COCO category_id(0~9)로 되돌림\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    w = x_max - x_min\n",
    "                    h = y_max - y_min\n",
    "                    results.append({\n",
    "                        'image_id': int(img_id),\n",
    "                        'category_id': int(label - 1),\n",
    "                        'bbox': [float(x_min), float(y_min), float(w), float(h)],\n",
    "                        'score': float(score)\n",
    "                    })\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()   # 여기서 mAP@[0.5:0.95] 등 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1) train/val DataLoader 준비\n",
    "    train_loader, val_loader, coco_val = build_loaders()\n",
    "\n",
    "    # 2) 디바이스, 모델, 옵티마이저\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "\n",
    "    num_classes = 11  # 10 classes + background\n",
    "    model = create_model(num_classes=num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params, lr=0.005, momentum=0.9, weight_decay=0.0005\n",
    "    )\n",
    "\n",
    "    # 3) 학습 (train_fn이 val까지 받도록 수정되어 있어야 함)\n",
    "    num_epochs = 12\n",
    "    train_fn(num_epochs, train_loader, val_loader, coco_val,\n",
    "             optimizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.09s)\n",
      "creating index...\n",
      "index created!\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /data/ephemeral/home/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:02<00:00, 69.7MB/s] \n",
      "100%|██████████| 306/306 [04:34<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 loss: 0.654836126353616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:33<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2 loss: 0.5142101999980951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:34<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3 loss: 0.4757283618247587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:33<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4 loss: 0.4529518393519657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:34<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5 loss: 0.43425681473675115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:33<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6 loss: 0.41960378923739483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:32<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7 loss: 0.40160997638028434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:32<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8 loss: 0.3920603515762909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:33<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9 loss: 0.3810855712668568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:34<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10 loss: 0.36799731767839855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:33<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11 loss: 0.35614126367993604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [04:32<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12 loss: 0.34828214505626487\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
